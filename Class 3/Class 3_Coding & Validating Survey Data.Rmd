---
title: "Survey Data: Coding & Validation"
author: "Miriam Sorace"
date: "18/11/2022"
output: pdf_document
---

# Initial SetUp

## 1. Global Settings

```{r, include=FALSE}
# include=FALSE means that this particular R code chunk will not be included in the final report. Useful for global settings or bits of code that might not be necessary to show in the final report (settings/set-up lines of code).
if (!("knitr" %in% installed.packages())) {
  install.packages('knitr', repos='http://cran.rstudio.org')}
library(knitr)

if (!("formatR" %in% installed.packages())) {
  install.packages("formatR")}
library(formatR)

knitr::opts_chunk$set(echo = TRUE, error = FALSE, warning = FALSE, message = FALSE, fig.align = "center", tidy.opts=list(width.cutoff=70), tidy=TRUE) 
#to avoid source code going out of bounds

#this line is used to specify any global settings to be applied to the R Markdown script.   The example sets all code chunks as “echo=TRUE”, meaning they will be included in the final rendered version, whereas error/warning messages or any other messages from R will not be displayed in the final, 'knitted' R Markdown file.

```

## 2. Set Working Directory

```{r, include=FALSE}
# Easy, manual option: go to Session --> Set Working Directory --> To Source File Location (this will mean that the computer folder where you have saved this RMarkdown file will be the working directory)

# Otherwise: copy/paste your folder path in the parentheses of the function below, deleting ~/Downloads

setwd("~/Downloads")

#e.g.

setwd("~/My Drive/Miriam Sorace_POLITICAL SCIENCE/7. MY TEACHING/Kent/MA5953/Class 3 Materials")
```

## 3. Install Required Packages & Call Relevant Libraries:

```{r}

if (!("tidyverse" %in% installed.packages())) {
  install.packages("tidyverse")}
library(tidyverse)

if (!("janitor" %in% installed.packages())) {
  install.packages("janitor")}
library(janitor)

if (!("ggplot2" %in% installed.packages())) {
  install.packages("ggplot2")}
library(ggplot2)


if (!("scales" %in% installed.packages())) {
  install.packages("scales")}
library(scales)

if (!("broom" %in% installed.packages())) {
  install.packages("broom")}
library(broom)


if (!("stargazer" %in% installed.packages())) {
  install.packages("stargazer")}
library(stargazer)

```

# Cognitive Interviewing

## 4. Load .csv of completed responses from Google Forms

Something to note: when downloading from Google Forms the variable name is the entire survey item, which can get unwieldy. Just easier to rename directly on the excel file and then upload on to R. Or you can always change the variable names with the line of code provided below, but the excel workaround will usually save you time (unless it is a long survey you are working with).

```{r}

MyCognInt<-read.csv("CognInterview.csv")

```


## 5. Coding/Data Cleaning

```{r} 

#generate ID variable
MyCognInt<-tibble::rowid_to_column(MyCognInt, "Identification No.")

#renaming variables, new name first, old name in-between quotes.
MyCognInt<-dplyr::rename(MyCognInt, ID = "Identification No.")

#dropping variables that are not needed
MyCognInt<-dplyr::select(MyCognInt, -Timestamp)


#re-coding empty cells and/or NAs **in the entire dataset**

MyCognInt[MyCognInt == "" | MyCognInt== " "] <- NA
MyCognInt[MyCognInt == "N/A" | MyCognInt == "n/a" | MyCognInt == "N/a"| MyCognInt == "-"] <- NA


#use class() and table() on the variables to see how they are stored in R

class(MyCognInt$QA)
table(MyCognInt$QA)
class(MyCognInt$QB)
table(MyCognInt$QB)


#Google Forms converts multiple choice to character: recode string/text/character variables, transform them to factor or numeric

simplify.likert <- function(x) {
  case_when(
    x == "Strongly Agree" ~ 4,
    x == "Agree" ~ 3,
    x == "Neutral" ~ 2,
    x == "Disagree" ~ 1,
    x == "Strongly Disagree" ~ 0,
    TRUE ~ NA_real_ #sets the rest (Don't Knows) to NA
  ) %>% factor(levels = c(0,1,2,3,4),
                labels = c('Strongly Disagree','Disagree','Neutral','Agree','Strongly Agree'))
}


#apply recoding function to the relevant variable. 
#create a new variable name, good to retain the old variable to check if the re-code functioned correctly!
MyCognInt <- MyCognInt %>%
  mutate(QA_rec = simplify.likert(QA))

#check whether the recode has worked
table(MyCognInt$QA, MyCognInt$QA_rec)

#do the same for QB (also an agree-disagree Likert scale in my case)

MyCognInt <- MyCognInt %>%
  mutate(QB_rec = simplify.likert(QB))

#check whether the recode has worked
table(MyCognInt$QB, MyCognInt$QB_rec)


# Difficulty questions are numeric but have no labels. To add value labels:

MyCognInt$QADiff <- ordered(MyCognInt$QA_CIprompt3,
                     levels = c(1,2,3,4,5),
                     labels = c("Not hard at all", "Somewhat hard", 
                                "Quite Hard", "Hard", "Very Hard"))

MyCognInt$QBDiff <- ordered(MyCognInt$QB_CIprompt3,
                     levels = c(1,2,3,4,5),
                     labels = c("Not hard at all", "Somewhat hard", 
                                "Quite Hard", "Hard", "Very Hard"))

#check recodes

table(MyCognInt$QA_CIprompt3, MyCognInt$QADiff)
table(MyCognInt$QB_CIprompt3, MyCognInt$QBDiff)


```

## 6. Validation: Answer Behavior in the Cognitive Probes

How often people chose the 'DK' option?  How often did they say that the question was too hard to answer (if analysing the cognitive interview probes)? Some simple descriptive tables/plots can help in getting a sense of how people answer the specific survey question. Providing some descriptive graphs is therefore a good first step. 


```{r}

#to get number of don't knows/refused in the survey items under investigation  (QA and QB in this example) run the summary function on the recoded variables, which will show how many NAs (if any are present)

summary(MyCognInt$QA_rec)
summary(MyCognInt$QB_rec)

#to assess how hard each question was perceived (cognitive interview prompt no.3), it might be nice to create a histogram or a bar chart of the answers

graphQADiff<-ggplot(data=MyCognInt, aes(x=QADiff)) +
  geom_bar(stat="count", width=0.7, fill="gray") +
  theme_minimal() +
  labs(title="How hard was the item: I have received helpful comments on my work.", 
         x=" ", y = "Count")

graphQADiff

summary(as.numeric(MyCognInt$QADiff))

graphQBDiff<-ggplot(data=MyCognInt, aes(x=QBDiff)) +
  geom_bar(stat="count", width=0.7, fill="gray") +
  theme_minimal() +
  labs(title=
         "How hard was the item: course quality satisfaction.", 
         x=" ", y = "Count")

graphQBDiff

summary(as.numeric(MyCognInt$QBDiff))



```


## 7. Cognitive Interviewing: Probing Comprehension and Recall

Cognitive interviewing by its nature has to be open-ended. To analyse answers to the open probes, content analysis of the answers' texts is required. We're going to see automated text analysis methods next term, at this stage and for the first report, you are supposed to do this more qualitatively. 

Go through each text from your files and try to systematize the answers into categories. When I have done this for the cognitive interview you filled out in class, the following patterns emerged.

### Helpful Feedback NSS Question 

#### Comprehension 

The main themes that emerged when students were asked to report what 'receiving helpful comments' means to them are *usable, practical advice on how to do the work better* (mentioned in 18 out of 20 answers), *praising comments* (mentioned in 5 out of 20 answers), and *clearly highlighting the mistakes* (mentioned in 5 out of 20 answers). Respondents seemed to understand helpful as re-usable, and many mentioned future-orientation in their answers. 

One respondent made this point clearly when complaining about `comments that are too specific to the individual piece of work and don't have much value going forward'. Another wrote: 'In all honesty, the majority of this feedback has been pretty useless to me as I am receiving it AFTER I have finished a module, where those particular skills were useful. Yes, there are some aspects which spread across different modules and assessments, but specific feedback rarely addressed this. Usual feedback was concerned with the content of a particular essay. I'm not going to rewrite the essay. I'm likely not going to look into the topic again in much detail during my degree. Therefore, it is hard to call most of the feedback useful.'

From this initial qualitative overview, 'helpful comments' means: general/multi-purpose/'future-proof' applied pieces of advice that simply signposts what was done well together with mistakes and that can be used in future work to achieve better grades in other modules/assessments.


#### Retrieval

Most respondents clearly outlined processes of recollection of past experiences (comparison of previous helpful and unhelpful feedback, verbal conversations with lecturers and academic advisors). Some mentioned that the word 'work' drove them to also include work experiences, rather than simply University assignments. This highlights a potential wording issue in the NSS.

### Recommendations

On the basis of the results from the cognitive interviewing on the survey item on helpfulness of the feedback, it appears that the NSS item works pretty well. It is not perceived as difficult to answer, and the memories recollected are in line with what would be expected. Also, the near consensus on the meaning of 'helpful feedback' is a suggestion that the item is generally understood in the same way by different respondents - hence having good reliability.

### Satisfaction with Course Quality NSS Question 

#### Comprehension 

The main themes that emerged when students were asked to report what 'University course quality' means to them are: (1) *content excellence*, including demonstrating research quality, benefiting from extensive expertise and applied knowledge of academic staff, and the amount of information disseminated (mentioned in 11 out of 20 responses); (2) *enjoable and passionate lecturers* (mentioned in 5 out of 20 responses); (3) *level of student support/responsivenes and approachability* (both academic and related to mental health - responses in this area often mention the availability of good student support teams), mentioned in 4 out of 20 responses; (4) *feasible/realistic workloads* mentioned in 3 out of 20 responses; (5) *utilitarian considerations*, including value for money and career-progression (mentioned in 5 out of 20 responses).


#### Retrieval

Most respondents clearly outlined processes of recollection of relevant past experiences (in particular, teaching delivery, teaching contents, clarity of module outlines and learning outcomes, feedback and grades). Some mentioned that retrieval also included estimations of the ease vs. difficulty of the content and how much they feel they have learnt as a result of the degree.

#### Recommendation

There is some agreement on the meaning of this question, with the content expertise and excellence of the academic staff theme being mentioned by more than half respondents. However, it is clear that students understand course quality in different ways and according to very specific priorities. This item was also scored as harder to answer. Clearly this NSS survey item needs some unpacking, and maybe consider specifying what is meant by quality or do a break-down for the various elements that make up a degree course of good quality (research excellence/applied teaching; inspirational teaching and feedback; student support and student services).

# Validation

## 8. Correlational Validity: Simple Visualisations

You will do this step as part of your final survey, not as part of your cognitive interviewing process. Correlational validity is fielding - in addition to your bespoke survey item - either a gold standard question or a survey item that should be a *theoretical correlate* (based on previous studies) of the phenomenon your bespoke survey item is supposed to measure. 

Let's load therefore a different dataset - containing the results from a made-up survey on political knowledge.

```{r}

FinalSurvey<-read.csv("survey.csv")

```

The dataset I have named 'Final Survey' contains 2 variables: PolKnow and PolInt. PolKnow contains a scale trying to capture political knowledge of the respondent, 10 meaning 'most knowledgeable' and 0 meaning 'least knowledgeable'. This is a new survey item that was fielded, and the researcher needs to now validate it against a 'theoretical correlate'.

The theoretical correlate chosen is PolInt which is a scale capturing how interested the respondent is in politics. 10 means they are very interested, 0 that they are not at all interested. Theoretically, people with high political interest should absorb more political information by virtue of superior exposure. 

So, if the (made-up) researcher's new (made-up) survey item is validly capturing political knowledge, it should be positively associated with political interest.


We can explore the associtation between the two variables first descriptively, via cross-tabulations and/or via scatterplots. See relevant lines of code below:

```{r}
#Cross-Tabulation (better for categorical variables)

table1 <- FinalSurvey %>%
  tabyl(PolKnow, #new survey item in rows, 
        PolInt #theoretical predictor in columns
        ) %>%
  adorn_totals("col") %>%
  adorn_percentages("col") %>% 
  adorn_pct_formatting(rounding = "half up", digits = 0) %>%
  adorn_ns() %>%
  knitr::kable()

table1

#Scatterplot (better for continuous variables 
#- can't use ordered/factor variables: use numerical! 
#If your variables are not numeric you can just convert them using 
#as.numeric()

#place theoretical predictor in x and new survey item in y

ggplot(FinalSurvey, aes(x=PolInt, y=PolKnow)) + 
  geom_point(color="black") +
  geom_smooth(method=lm, linetype="dashed",
             color="gray29") +
  theme_classic() 


```

If we look at the table, when Political Interest has higher values, Political Knowledge also has higher values. The scatterplot also clearly shows a positive association: the more the respondents report political interest, the more knowledgeable they are.

## 9. Correlational Validity via Formal Hypothesis Testing

The above was just a way of 'eyeballing' the relationship. For a formal evaluation of the relationship, we can use regression analysis. Regression analysis is a statistical technique that estimates the strength and statistical significance of the association between two (or more) variables. For more information, see: https://stats.idre.ucla.edu/wp-content/uploads/2021/05/R_reg_part1.html#(1)

We won't go into the details here, it will just be sufficient to know that the coefficient next to the second variable is the amount of change in the first variable (the dependent variable - i.e. our new survey item that we are trying to validate) for a 1-unit change in the explanatory (or theoretical predictor) variable. The second thing to know is that if the t-value is above 2, the relationship is statistically significantly different from zero - meaning that an effect of zero (absence of a relationship) is statistically unlikely given the data observed. Whenever the t-value is above 2, the regression table will display stars. 

```{r, results='asis'}
#remember: you need to add ,results='asis' for stargazer to output a table 
#in your knitted Rmd

model1<-lm(PolKnow ~ PolInt, data = FinalSurvey)

summary(model1)

#getting a nicer-looking table

stargazer(model1)




```

We find that the slope (beta) coefficient for Political Interest is ~ 0.99 and statistically significant at the 0.001 level. This means that the relationship is statistically significantly different from zero. The value of 0.99 means that for a unit increase in political interest there is nearly a unit (0.99 of a unit) increase in political knowledge! Given that the two items are measured using the same scale (disagree-agree in 5-points Likert scales) this is nearly a 1:1 relationship, so quite strong. This is really the best case scenario for validation (and highly unlikely! Just a feature of my fake data :) )  

REMEMBER: if the two variables are not measured using the same scale, you won't be able to interpret the magnitude of the effect well like in this case. You'll need to standardise your variables first in order to do compare the two variables fully (see below):


```{r, results='asis'}
#extra code: in case you'll need to correlate two variables measured with different scales

modelStd<-lm(scale(PolKnow) ~ scale(PolInt), data=FinalSurvey)

#getting a nicer-looking table

stargazer(modelStd)


```

This will return the coefficient expressed in standard deviations change: for a standard deviation increase in X, you get [beta coefficient value] standard deviation increase in Y. 
