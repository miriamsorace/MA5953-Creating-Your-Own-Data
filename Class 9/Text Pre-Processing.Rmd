---
title: "Text Pre-Processing Code"
output: html_document
---


```{r, include=FALSE}

#### GLOBAL OPTIONS (REPORT RENDERING)

# include=FALSE means that this particular R code chunk will not be included in the final report. Useful for global settings or bits of code that might not be necessary to show in the final report (settings/set-up lines of code).
if (!("knitr" %in% installed.packages())) {
  install.packages('knitr', repos='http://cran.rstudio.org')}
library(knitr)

if (!("formatR" %in% installed.packages())) {
  install.packages("formatR")}
library(formatR)

knitr::opts_chunk$set(echo = TRUE, error = FALSE, warning = FALSE, message = FALSE, fig.align = "center",
                      tidy.opts=list(width.cutoff=70), tidy=TRUE) #to avoid source code going out of bounds
                                                                  #does not work if single var name is too
                                                                  #long though, as in point 5 below!  

#this line is used to specify any global settings to be applied to the R Markdown script.   The example sets all code chunks as “echo=TRUE”, meaning they will be included in the final rendered version, whereas error/warning messages or any other messages from R will not be displayed in the final, 'knitted' R Markdown file.

### permanently setting the CRAN mirror (avoids error "trying to use CRAN without setting a mirror")

local({r <- getOption("repos")
       r["CRAN"] <- "http://cran.r-project.org"
       options(repos=r)})

```




```{r, include = F}

#### INSTALL RELEVANT PACKAGES

if (!("quanteda" %in% installed.packages())) {
  install.packages("quanteda")
}

if (!("quanteda.textmodels" %in% installed.packages())) {
  install.packages("quanteda.textmodels")
}

if (!("quanteda.textplots" %in% installed.packages())) {
  install.packages("quanteda.textplots")
}

if (!("quanteda.textstats" %in% installed.packages())) {
  install.packages("quanteda.textstats")
}

if (!("devtools" %in% installed.packages())) {
  install.packages("devtools")
}

if (!("quanteda.corpora" %in% installed.packages())) {
  devtools::install_github("quanteda/quanteda.corpora")
}

if (!("readtext" %in% installed.packages())) {
  install.packages("readtext")
}

if (!("dplyr" %in% installed.packages())) {
  install.packages("dplyr")
}

if (!("tidytext" %in% installed.packages())) {
  install.packages("tidytext")
}

if (!("ggplot2" %in% installed.packages())) {
  install.packages("ggplot2")
}

if (!("rtweet" %in% installed.packages())) {
  install.packages("rtweet")
}

require(quanteda)
require(quanteda.textmodels)
require(quanteda.textplots)
require(quanteda.textstats)
require(quanteda.corpora)
require(readtext)
require(dplyr)
require(tidytext)
require (ggplot2)
require(rtweet)

```


```{r, include=F}
##### REMEMBER: WORKING DIRECTORY!

# Either Session --> Set Working Directory --> To Source File Location (should do it automatically, but only when knitting - so, if you want to run the code bit by bit first do set the wd here). To Source File Location means that the data will be imported and saved IN THE SAME FOLDER where you saved the RMarkdown file you're working with!

## OR

## setwd("")

```


## Task 1: Reading Text Data into R & Transforming to a Corpus Object ##

"Corpus" is a term from linguistics and it indicates a collection of documents. A Corpus class object in R organises your text data and all the relevant meta-data allowing subsequent easy processing of texts and text-descriptive variables (meta-data - e.g. author, dates, etc ...). A necessary first step in text mining is therefore to transform the datafiles containing your texts and meta-data (usually stored in a .csv) into a corpus object, as done below:

```{r}
### User 1:

ObamaTweets<-readtext("Obama_tweets.csv", text_field = "text", encoding = "ISO-8859-1")

#Twitter-specific step: remove emoticons (using regular expression for emojis/non-ASCII characters):
ObamaTweets$text <-gsub("[^\x01-\x7F]", "", ObamaTweets$text)

ObamaCorpus<-corpus(ObamaTweets)


### User 2: 

PenceTweets<-readtext("Pence_tweets.csv", text_field = "text", encoding = "ISO-8859-1")

#Twitter-specific step: remove emoticons (using regular expression for emojis/non-ASCII characters):
PenceTweets$text <-gsub("[^\x01-\x7F]", "", PenceTweets$text)

PenceCorpus<-corpus(PenceTweets)

```

Note that instead of using read.csv we have to use the readtext function and specify what is the text column in our .csv file. This will make sure that R will distinguish the text column from the other columns in the file containing meta-data information.

## Task 2: Initial Descriptives ##

Using the plain summary(corpus) line of code will only show the first 100 docs and all metadata. It is useful as a first check of your data. To retrieve useful information summarising total number of words used (tokens) or total number of unique words used (types) or total sentences you should first save the summary(corpus) as an object and extract its relevant variables. 

```{r, include =F}
summary(ObamaCorpus)
summary(PenceCorpus)
```

```{r}
#total number of documents in each corpus:

ndoc(ObamaCorpus)
ndoc(PenceCorpus)

# types, tokens, sentence summaries

sumObamacorp<-summary(ObamaCorpus, n=3200)
summary(sumObamacorp$Tokens)
summary(sumObamacorp$Types)
summary(sumObamacorp$Sentences)


sumPencecorp<-summary(PenceCorpus, n=3200)
summary(sumPencecorp$Tokens)
summary(sumPencecorp$Types)
summary(sumPencecorp$Sentences)

#If you want, you can also save this summary outputs in a csv file:

write.csv(sumObamacorp, file="Obama_Summary.csv", row.names=FALSE)
write.csv(sumPencecorp, file="Pence_Summary.csv", row.names=FALSE)

#To visualise a specific text use the text() extractor function, as below:
ObamaTweets$text[1000]
```


## Task 3: Dealing with Metadata ##

Now that we have imported our texts, we may need to 'clean' some of the metadata variables that we might use later, by renaming them or reshaping the date/time variables and so on ... The code below tells R to use a particular corpus metadata object into a proper variable that we can use for analysis, e.g. the count of retweets for each tweet, or the date of creation of each tweet. The function **docvars** retrieves desired metadata columns and is used every time we want to transform a corpus variable (renaming it, splitting it or doing other operations on it). Applying docvars will create a new variable in the corpus with the required transformation.

```{r}
#use function docvars to rename variables as needed
ObamaCorpus$retweets<-docvars(ObamaCorpus, "retweet_count")
ObamaCorpus$timestamp<-docvars(ObamaCorpus, "created_at")

PenceCorpus$retweets<-docvars(PenceCorpus, "retweet_count")
PenceCorpus$timestamp<-docvars(PenceCorpus, "created_at")

```


## Task 4: Additional Descriptives ##

```{r}

### OVER-TIME: we see that Obama tweets go back to 2016, while Pence tweets are only from 2020 onwards

ts_plot(ObamaTweets, "24 hours") +
  ggplot2::theme_minimal() +
  ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
  ggplot2::labs(
    x = NULL, y = NULL,
    title = "Frequency of Tweets by Barack Obama over time",
    subtitle = "Twitter status (tweet) counts by 24 hour intervals",
    caption = "\nSource: Data collected from Twitter's REST API via rtweet" )


ts_plot(PenceTweets, "24 hours") +
  ggplot2::theme_minimal() +
  ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
  ggplot2::labs(
    x = NULL, y = NULL,
    title = "Frequency of Tweets by Mike Pence over time",
    subtitle = "Twitter status (tweet) counts by 24 hour intervals",
    caption = "\nSource: Data collected from Twitter's REST API via rtweet" )


# EXTRA CODE: for all subsequent analyses, if you wish, you can subset the corpus, so that both actors are compared across the same period. To do this, use the timestamp variable in the corpus

ObamaCorpusSubset <- corpus_subset(ObamaCorpus, timestamp >= 2020)
ndoc(ObamaCorpusSubset)

#you will notice though that you'll have much less tweets from Obama to work with.

```


## Task 5: Keywords in Context ##

Another important step of the descriptive analysis of text data is to explore how particular words/concepts that you might be interested in are used. KWICs - Keywords in Context - show the word of interest (keyword) embedded in a number of precedent and subsequent words. The lines of code below show how to visualise KWICs and how to specify a word 'window'.

```{r}
#exploring KWICs

kwic(tokens(PenceCorpus), pattern = "job", window=3)
kwic(tokens(ObamaCorpus), pattern = "job", window=3)


#to explore all words containing the pattern
kwic(tokens(PenceCorpus), pattern = "job", valuetype="regex", window=3)
#or
kwic(tokens(PenceCorpus), pattern = "*job*", window=3)

#multi-word KWICs
kwic(tokens(ObamaCorpus), pattern = phrase("United States"), window=3)

```


## Task 6: Document-Feature Matrices & Text Data Cleaning ##

The document-feature matrix is the building block of any text-mining exercise. The code below converts a corpus object into a dfm object. Moreover, the code below allows to 'pre-process' the text data in the corpus. Dfm object options such as "remove=" or "stem=" allow to, for example, eliminate stop-words, punctuation and to stem words from Tweets.

```{r}

DFM_Obama<- tokens(ObamaCorpus, 
                   remove_punct = TRUE, 
                   remove_symbols = T, 
                   remove_numbers=T, 
                   remove_url=T) %>%
                   dfm() %>%
                   dfm_tolower()


#check top words:
topfeatures(DFM_Obama, n = 20)


DFM_Pence<- tokens(PenceCorpus, 
                   remove_punct = TRUE, 
                   remove_symbols = T, 
                   remove_numbers=T, 
                   remove_url=T) %>%
                   dfm() %>%
                   dfm_tolower()

#check top words:
topfeatures(DFM_Pence, n = 20)


#It looks like stopwords are taking centre-stage. Let's explore stopword lists & then eliminate them from our dfm, as unlikely to be useful in text analysis (may only capture rhetorical style).

#exploring stopwords
head(stopwords("en"), 100)
head(stopwords("it"), 100)
head(stopwords("fr"), 100)

#to remove stopwords:
DFM_Obama<- dfm_remove(DFM_Obama, pattern = stopwords("en"))
#check top words:
topfeatures(DFM_Obama, n = 20)


DFM_Pence<- dfm_remove(DFM_Pence, pattern = stopwords("en"))
#check top words:
topfeatures(DFM_Pence, n = 20)

# to remove other words you think irrelevant (stopword lists may not be comprehensive)

DFM_Obama <- dfm_remove(DFM_Obama, pattern = c("one", "also", "obama", "president", "amp"))
DFM_Pence <- dfm_remove(DFM_Pence, pattern = c("one", "also", "pence", "president", "amp", "@mike_pence"))


```

## Task 7: Stemming ##

Stemming is a step that brings all tokens to their root form (see: https://quanteda.io/reference/tokens_wordstem.html). This helps in reducing the sparsity of the dfm and improve computational efficiency. You need to think carefully about this as stemming may hide meaningful variation (e.g. police and policy will become the same type, as their stem is the same).

```{r}
DFM_Obama_stem <- dfm_wordstem(DFM_Obama)
DFM_Pence_stem <- dfm_wordstem(DFM_Pence)
```


## Task 8: Term Frequency, Inverse Document Frequency Weighting ##

Sometimes you might wish to 'weight' words according to their frequency within documents and across the corpus. This is achieved by the tf-idf measure. The dfm object can accommodate - via the "scheme" option - this additional transformation. See the code below for the implementation of tf-idf weighting in a dfm. This is not recommended if the aim of your text analysis involves scaling (e.g. Benoit 2020), but can help the computational efficiency of text classifiers.

```{r}
#tf-idf

DFM_Obama_tfidf<- DFM_Obama %>%
  dfm_tfidf( scheme_tf = "count", scheme_df = "inverse",base = 10) 


DFM_Pence_tfidf<- DFM_Pence %>%
  dfm_tfidf( scheme_tf = "count", scheme_df = "inverse",base = 10) 

```


## Task 9: Text Descriptives: WORDCLOUDS ## 

We introduced wordclouds last week: a wordcloud is a visualisation of the most common words in a corpus of texts. The function **textplot_wordcloud** from quanteda is helpful in building wordclouds. The function has several options that allows to customise the look of the wordcloud. For more options see: https://quanteda.io/reference/textplot_wordcloud.html

```{r}
# wordcloud

textplot_wordcloud(DFM_Obama, 
                   min_count = 5,
                   max_words = 150,
                   min_size = 0.25,
                   max_size = 3,
                   random_order = FALSE, 
                   rotation = 0.25,
                   color = "darkblue")


textplot_wordcloud(DFM_Pence, 
                   min_count = 5,
                   max_words = 150,
                   min_size = 0.25,
                   max_size = 3,
                   random_order = FALSE, 
                   rotation = 0.25,
                   color = "darkred")



```



**Task 9: Group Analysis**

It is possible to collate the two different corpora and then run a comparative analysis by splitting the dfm by author. This is what the code below does.

```{r}
#adding two corpus objects together

MasterCorpus<-ObamaCorpus+PenceCorpus
MasterCorpus$author<-docvars(MasterCorpus, "screen_name")

#Do all the relevant cleaning steps above using the "Master Corpus":


DFM_Master<- tokens(MasterCorpus, 
                   remove_punct = TRUE, 
                   remove_symbols = T, 
                   remove_numbers=T, 
                   remove_url=T) %>%
                   dfm() %>%
                   dfm_tolower()


#check top words:
topfeatures(DFM_Master, n = 20)


# remove stopwords:
DFM_Master<- dfm_remove(DFM_Master, pattern = stopwords("en"))
#check top words:
topfeatures(DFM_Master, n = 20)


# to remove other words you think irrelevant (stopword lists may not be comprehensive)

DFM_Master <- dfm_remove(DFM_Master, pattern = c("one", "also", "president", "amp", "obama", "@mike_pence"))




# EXTRA CODE: to check word frequencies by author (keyness):

prop_dfm_byauthor<- tokens(MasterCorpus,
                           remove_punct = TRUE,
                           remove_symbols = T, 
                           remove_numbers=T,    
                           remove_url=T) %>%
                           dfm() %>%
                           dfm_tolower() %>%
                           dfm_remove(pattern = stopwords("en")) %>%
                           dfm_remove(pattern = c("one", "also", "president", "amp", "obama",
                                                  "@mike_pence")) %>%
                          dfm_group(groups = author) %>%
                          dfm_weight(scheme = "prop")


write.csv(convert(prop_dfm_byauthor, to = "data.frame"), file="dfm_group.csv", row.names=FALSE)
    

```

## Task 10: Analysing Author Differences in a Single Plot ##

To inspect and plot differences in word usage between two authors, see the lines of code below:

```{r}

#we can sort and inspect groups

DFM_Master_Group<-dfm_sort(DFM_Master)

#use words up to 20th rank

DFM_Master_Group<-textstat_frequency(
  DFM_Master_Group,
  n = 20,
  groups = author)

#plot for comparison (using dfm with top 20th ranked words only)

ggplot(data = DFM_Master_Group, aes(x = factor(nrow(DFM_Master_Group):1), y = frequency)) +
    geom_point() +
    facet_wrap(~ group, scales = "free") +
    coord_flip() +
    scale_x_discrete(breaks = nrow(DFM_Master_Group):1,
                       labels = DFM_Master_Group$feature) +
    labs(x = NULL, y = "Relative frequency")


```


## Lab Exercise ##

Play around with the code above and with the quanteda tutorials (here: ) using your scraped Tweets. This will form part of your assignment, so good to get started now!

```{r}


```


