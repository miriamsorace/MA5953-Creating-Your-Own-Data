---
title: "Problem Set - Class 7"
output: html_document
---
**Web Scraping with R**

## Global Options & Cran Mirrors ##

```{r, include=FALSE}
# include=FALSE means that this particular R code chunk will not be included in the final report. Useful for global settings or bits of code that might not be necessary to show in the final report (settings/set-up lines of code).
if (!("knitr" %in% installed.packages())) {
  install.packages('knitr', repos='http://cran.rstudio.org')}
library(knitr)

if (!("formatR" %in% installed.packages())) {
  install.packages("formatR")}
library(formatR)

knitr::opts_chunk$set(echo = TRUE, error = FALSE, warning = FALSE, message = FALSE, fig.align = "center",
                      tidy.opts=list(width.cutoff=70), tidy=TRUE) #to avoid source code going out of bounds
                                                                  #does not work if single var name is too
                                                                  #long though, as in point 5 below!  

#this line is used to specify any global settings to be applied to the R Markdown script.   The example sets all code chunks as “echo=TRUE”, meaning they will be included in the final rendered version, whereas error/warning messages or any other messages from R will not be displayed in the final, 'knitted' R Markdown file.

### permanently setting the CRAN mirror (avoids error "trying to use CRAN without setting a mirror")

local({r <- getOption("repos")
       r["CRAN"] <- "http://cran.r-project.org"
       options(repos=r)})

```

## Install and call the relevant packages ##

```{r}
if (!("dplyr" %in% installed.packages())) {
  install.packages("dplyr")
}
library(dplyr)

# General-purpose data wrangling
if (!("tidyverse" %in% installed.packages())) {
  install.packages("tidyverse")
}
library(tidyverse)  

# Parsing of HTML/XML files  
if (!("rvest" %in% installed.packages())) {
  install.packages("rvest")
}
library(rvest)    

# String manipulation
if (!("stringr" %in% installed.packages())) {
  install.packages("stringr")
}
library(stringr)   

# Verbose regular expressions
if (!("rebus" %in% installed.packages())) {
  install.packages("rebus")
}
library(rebus)     

# Eases DateTime manipulation
if (!("lubridate" %in% installed.packages())) {
  install.packages("lubridate")
}
library(lubridate)

if (!("readr" %in% installed.packages())) {
  install.packages("readr")
}
library(readr)

```


## Task 2 (optional with RMarkdown). Setting up your working directory ##

NB: this step is not strictly necessary if you use RMarkdown (can be useful to have if it gives you a `cannot find filepath' type of errors, though!): everything will be automatically saved where the RMarkdown file is, so it is important to make sure you place your RMarkdown file in the folder (working directory) you want all your data to be saved in.

If you are not using RMarkdown, or you want to separate RMarkdown from your files directory, use the command below (remove the hashtag) and place your filepath between the quotes:

```{r}
#setwd("")
```



## Task 3. Scraping Tables ##

```{r}
#parse the webpage
url<-"https://finance.yahoo.com/quote/AAPL/history?p=AAPL"
apple_finance<- read_html(url)
apple_finance

#html_table function retrieves the table tag in the webpage & transforms to list
tableaslist<-html_table(apple_finance)

#to transform to readable data-frame:
table<-as.data.frame(tableaslist)

#to show the first ten rows of the scraped table:
table[1:10,]

#to see the full table you can go in the R environment (top right corner) and click on the object called 'table'
```

## Task 4. Scraping Text ##

html_text is the function to retrieve text
Use Chrome as the browser and download SelectorGadget extension
Go to: https://www.europarl.europa.eu/doceo/document/CRE-9-2020-06-17-INT-1-161-0000_EN.html
Click on the selector gadget icon and then on the relevant section to scrape

SelectorGadget will give you the node name to input into html_nodes in this case **.content** seems the selector that identifies the entire speech text

```{r}
#### scraping content (text, numbers...) , not as easy as in the table case, this needs a bit more of our input

url<-"https://www.europarl.europa.eu/doceo/document/CRE-9-2020-06-17-INT-1-161-0000_EN.html"
v_speech1<-read_html(url)
v_speech1

text <- v_speech1 %>% 
  html_nodes(".contents") %>%
  html_text() %>%
  as.data.frame()

#to see all rows (each paragraph is scraped as a separate row)
text

#to see the first row (i.e. text paragraph)
text[1,]

# to place the entirety of the text in one cell only:

text<-as.data.frame(paste(text, collapse =" "))

#view text (again, you can also check by clicking the text object in the environment on the top right)
text
```


**Task 5. Scraping lists & links**

In here, the selector that identifies our content of interest is not one but two selectors are involved, since we want to avoid text content that is outside the list of speeches. Therefore, in this particular case, we need 2 nodes: the section containing the list of speeches, and .t-items within it
Double nodes can be accommodated by the **html_nodes** function: you can simply add each selector next to each other

```{r}
url<-"https://www.europarl.europa.eu/meps/en/97058/GUY_VERHOFSTADT/main-activities/plenary-speeches#detailedcardmep"
verhof_speech <- read_html(url)
verhof_speech



speech_list <- verhof_speech %>%
  html_nodes(".erpl_search-results-list-expandable-block .t-item") %>%
  html_text() %>%
  as.data.frame()

#give name to var
names(speech_list)[1] <- 'title'

#now let's retrieve the links, may be useful in the future :)
speech_links <- verhof_speech %>%
  html_nodes(".erpl_search-results-list-expandable-block a") %>% 
  html_attr("href") %>%
  as.data.frame()
names(speech_links)[1] <- 'link'


#EXTRA: if last row does not contain link (e.g. a string starting with 'www'), to drop it do the following:
#speech_links <- as.data.frame(speech_links[grepl("www", speech_links$url),])
#names(speech_links)[1] <- 'url'


# combine the speech titles with the links:

speech_data<-data.frame (speech=speech_list, link = speech_links)

```

## Task 6: Looping through URLs ##

```{r}
#### loop through URLs

MEPcodes <- c("97058/GUY_VERHOFSTADT", "197395/ALICE_KUHNKE", "96922/IZASKUN_BILBAO+BARANDICA")

urls <- paste0("https://www.europarl.europa.eu/meps/en/", MEPcodes , "/main-activities/plenary-speeches#detailedcardmep" )

#create an empty "catcher" list, where your scraped content will go
catcherlist<-list()

for (i in urls) {
 page <- read_html(i)
 Name <- page %>% html_nodes("#presentationmep .erpl_title-h1") %>% html_text() %>% as.character()
 Title <- page %>% html_nodes(".erpl_search-results-list-expandable-block .t-item") %>% html_text() %>% as.character()
 Link <- page %>% html_nodes(".erpl_search-results-list-expandable-block a") %>% html_attr("href") %>% as.character()
 temp <- list(Name, Title, Link)
  catcherlist <- rbind(catcherlist,temp)
}

#transform to usable data frame:
df<-as.data.frame(catcherlist)

#rename variables of the dataframe:
names(df)[1] <- 'MEP'
names(df)[2] <- 'SpeechTitle'
names(df)[3] <- 'Url'

#the dataframe created is a 3 row table - all speeches and links for each MEP are embedded in a list, not in separate rows.
#to deal with this, you can `unpack' the lists and expand the dataframe with the below:

df0<-unnest(df, cols=c(SpeechTitle, Url))

#to visualise the dataset in a nice table when the document knits:
kable(df0)

```


## Task 7: Scrape Text Following a List of Links ##

```{r}
##follow links

#Setup empty data frame

catcher_text <- data.frame(Name=character(),Text=character())


for (i in df0$Url) {
  
  page <- read_html(i)
  Name<-page %>% html_nodes(".doc_subtitle_level1_bis .bold") %>% html_text() %>% as.character()
  Text<-page %>% html_nodes(".contents") %>% html_text() %>% as.character()
  temp_text <- data.frame(Name, Text) #fill temporary repository
  catcher_text <- rbind(catcher_text,temp_text) #convert into dataframe
}

#rename the dataframe:
MEPspeeches<-catcher_text

#export to csv:

write_excel_csv(MEPspeeches, file="MEPspeeches.csv")

```

## Independent Exercises ##

1. Go to this url: https://www.ons.gov.uk/economy/inflationandpriceindices/bulletins/consumerpriceinflation/december2022
2. Play around with the webpage: what does it contain?
3. Scrape Table 1, input your code below 

(tip: Task 3 provides the answer, as the webpage has 3 tables though you might need to tweak the task 3 code line 5 like so: table<-as.data.frame(tableaslist[1])

```{r}

```

4. Was it necessary to build a scraper for this task? Could the data have been retrieved by other means?

5. The resulting data-frame has awkward names, rename all columns. Input the code below (tip: Task 5 has some relevant lines of code):

```{r}

```


